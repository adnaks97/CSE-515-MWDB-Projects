{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from scipy.spatial import distance\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'phase2_outputs/'\n",
    "mode = 'tfidf' #tf \n",
    "file_list = glob.glob('phase2_outputs/task0b/{}_*.txt'.format(mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [json.loads(json.load(open(fname, 'r'))) for fname in sorted(file_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5953"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 88, 249, 115, 207, 229, 253, 131,  47,  42, 197,  39,  11,  58,\n",
       "       132, 175, 235, 112, 141, 213, 179, 124, 103,  56,  77, 178, 172,\n",
       "       245, 189, 174,  83, 123, 216, 135, 184, 111, 204, 239, 171, 126,\n",
       "       204,  75, 169, 175, 189,  61, 252,   0, 120,  41, 215,  13, 249,\n",
       "       202, 235, 142, 232, 204, 127, 189,  56, 119, 235, 228, 218, 130,\n",
       "       168, 170, 203,  98, 234, 221, 206, 237,   7, 227, 230, 175, 169,\n",
       "        53,  43,   8, 183, 128, 107, 174, 250, 140,  45, 202, 188, 111,\n",
       "        25,  50])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs = np.random.randn(len(data[0]), 8)\n",
    "bin_vec = np.array(data).dot(vecs) >= 0\n",
    "exponents = np.array([2**i for i in range(7, -1, -1)])\n",
    "bin_vec.dot(exponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45394"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_vec[0].dot(exponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    def __init__(self, L, k, mode, input_dir):\n",
    "        self.L= L\n",
    "        self.k = k\n",
    "        self.mode = mode\n",
    "        self.data = None\n",
    "        self.hash_tables = []\n",
    "        self.n_words = None\n",
    "        self.file_list = sorted(glob.glob('{}/{}_*.txt'.format(input_dir, mode)))\n",
    "        self.idx_2_file = {i:fname for i,fname in enumerate(self.file_list)}\n",
    "        self.file_2_idx = {fname:i for i,fname in enumerate(self.file_list)}\n",
    "    \n",
    "    \n",
    "    def get_random_vectors(self):\n",
    "        return np.random.randn(self.n_words, self.k)\n",
    "    \n",
    "    def load_data(self):\n",
    "        self.data = np.array([json.loads(json.load(open(fname, 'r'))) for fname in self.file_list])\n",
    "        self.n_words = len(self.data[0])\n",
    "    \n",
    "    def binary_2_integer(self, binary_vectors):\n",
    "        exponents = np.array([2**i for i in range(self.k - 1, -1, -1)])\n",
    "        return binary_vectors.dot(exponents)\n",
    "    \n",
    "    def hash_data(self, data, random_vectors):\n",
    "        binary_repr = data.dot(random_vectors) >= 0\n",
    "        binary_inds = self.binary_2_integer(binary_repr)\n",
    "        return binary_inds\n",
    "    \n",
    "    def train(self):\n",
    "        self.load_data()\n",
    "        for i in range(self.L):\n",
    "            random_vectors = self.get_random_vectors()\n",
    "            binary_inds = self.hash_data(self.data, random_vectors)\n",
    "            table = defaultdict(list)\n",
    "            for idx, bin_ind in enumerate(binary_inds):\n",
    "                table[bin_ind].append(idx)\n",
    "            hash_table = {'random_vectors': random_vectors, 'table': table}\n",
    "            self.hash_tables.append(hash_table)\n",
    "    \n",
    "    def query(self, data_point, max_results):\n",
    "        retrieval = set()\n",
    "        n_buckets = 0\n",
    "        n_candidates = 0\n",
    "        for hash_table in self.hash_tables:\n",
    "            table = hash_table['table']\n",
    "            random_vectors = hash_table['random_vectors']\n",
    "            binary_idx = self.hash_data(data_point, random_vectors)\n",
    "            n_buckets += 1\n",
    "            #print(binary_idx, table[binary_idx])\n",
    "            retrieval.update(table[binary_idx])\n",
    "        \n",
    "        retrieval = list(retrieval)\n",
    "        sim_scores = cosine_similarity(np.expand_dims(data_point, 0), self.data[retrieval]).ravel()\n",
    "        data_idx = sim_scores.argsort()[::-1][:max_results]\n",
    "        #print(sim_scores)\n",
    "        #print(data_idx)\n",
    "        #print(retrieval)\n",
    "        assert len(retrieval) == len(sim_scores)\n",
    "        return {'n_buckets': n_buckets, \n",
    "                'n_candidates': len(retrieval),\n",
    "                'scores': sim_scores[data_idx],\n",
    "                'retrieved_files': [self.idx_2_file[retrieval[d]] for d in data_idx]\n",
    "               }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh = LSH(L=8, k=4, mode='tfidf', input_dir='phase2_outputs/task0b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query file:  phase2_outputs/task0b/tfidf_vectors_251.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_buckets': 8,\n",
       " 'n_candidates': 60,\n",
       " 'scores': array([1.        , 0.28351168, 0.27296928, 0.26864773, 0.25547429,\n",
       "        0.25533848, 0.25117153, 0.24767286, 0.24667712, 0.24310759]),\n",
       " 'retrieved_files': ['phase2_outputs/task0b/tfidf_vectors_251.txt',\n",
       "  'phase2_outputs/task0b/tfidf_vectors_249.txt',\n",
       "  'phase2_outputs/task0b/tfidf_vectors_561.txt',\n",
       "  'phase2_outputs/task0b/tfidf_vectors_257.txt',\n",
       "  'phase2_outputs/task0b/tfidf_vectors_261.txt',\n",
       "  'phase2_outputs/task0b/tfidf_vectors_018.txt',\n",
       "  'phase2_outputs/task0b/tfidf_vectors_007.txt',\n",
       "  'phase2_outputs/task0b/tfidf_vectors_562.txt',\n",
       "  'phase2_outputs/task0b/tfidf_vectors_013.txt',\n",
       "  'phase2_outputs/task0b/tfidf_vectors_250.txt']}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_idx = 33\n",
    "print(\"Query file: \", lsh.idx_2_file[query_idx])\n",
    "lsh.query(np.array(data[query_idx]), max_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
