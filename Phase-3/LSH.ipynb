{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from scipy.spatial import distance\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'phase2_outputs/task0b'\n",
    "mode = 'tfidf' #tf \n",
    "file_list = glob.glob('phase2_outputs/task0b/{}_*.txt'.format(mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataMatrix(input_dir, mode):\n",
    "    file_list = glob.glob(input_dir + '/{}_*.txt'.format(mode))\n",
    "    data = dict()\n",
    "    for fname in sorted(file_list):\n",
    "        fileNum = int(fname.split(\"_\")[-1].split('.')[0])\n",
    "        data[fileNum] = json.loads(json.load(open(fname, 'r')))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadDataMatrix(input_dir, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    def __init__(self, L, k, mode, input_dir):\n",
    "        self.L= L\n",
    "        self.k = k\n",
    "        self.mode = mode\n",
    "        self.data = None\n",
    "        self.hash_tables = []\n",
    "        self.n_words = None\n",
    "        self.file_list = sorted(glob.glob('{}/{}_*.txt'.format(input_dir, mode)))\n",
    "        self.idx_2_file = {i:fname for i,fname in enumerate(self.file_list)}\n",
    "        self.file_2_idx = {fname:i for i,fname in enumerate(self.file_list)}\n",
    "    \n",
    "    \n",
    "    def get_random_vectors(self):\n",
    "        \"\"\"Generates a random array of num_words*k dimension\"\"\"\n",
    "        return np.random.randn(self.n_words, self.k)\n",
    "    \n",
    "    def load_data(self):\n",
    "        self.data = np.array([json.loads(json.load(open(fname, 'r'))) for fname in self.file_list])\n",
    "        self.n_words = len(self.data[0])\n",
    "    \n",
    "    def binary_2_integer(self, binary_vectors):\n",
    "        exponents = np.array([2**i for i in range(self.k - 1, -1, -1)])\n",
    "        return binary_vectors.dot(exponents)\n",
    "    \n",
    "    def hash_data(self, data, random_vectors):\n",
    "        if(len(data.shape)==1):\n",
    "            data = data.reshape(1,data.shape[0])\n",
    "        binary_repr = data.dot(random_vectors) >= 0\n",
    "        #binary_inds = self.binary_2_integer(binary_repr)\n",
    "        binary_string_arr = []\n",
    "        for idx, binaryArr in enumerate(binary_repr.astype(int).astype(str)):\n",
    "            binary_string_arr.append(str.encode(''.join(binaryArr)))\n",
    "        return binary_string_arr\n",
    "    \n",
    "    def train(self):\n",
    "        self.load_data()\n",
    "        for i in range(self.L):\n",
    "            random_vectors = self.get_random_vectors()\n",
    "            binary_inds = self.hash_data(self.data, random_vectors)\n",
    "            table = defaultdict(list)\n",
    "            for idx, bin_ind in enumerate(binary_inds):\n",
    "                table[bin_ind].append(idx)\n",
    "            hash_table = {'random_vectors': random_vectors, 'table': table}\n",
    "            self.hash_tables.append(hash_table)\n",
    "            \n",
    "            \n",
    "    def query(self, data_point, max_results):\n",
    "        retrieval = set()\n",
    "        n_buckets = 0\n",
    "        n_candidates = 0\n",
    "        permLevel = 0\n",
    "        \n",
    "        while(len(retrieval) < max_results):\n",
    "            for hash_table in self.hash_tables:\n",
    "                table = hash_table['table']\n",
    "                random_vectors = hash_table['random_vectors']\n",
    "                binary_idx = self.hash_data(data_point, random_vectors)[0]\n",
    "                \n",
    "                # Neighbors of permutations of the original hash\n",
    "                if(permLevel>0):\n",
    "                    for hash_table_idx in table.keys():\n",
    "                        # counting changed bits\n",
    "                        xorNum = bin(int(binary_idx,2) ^ int(hash_table_idx,2))[2:]\n",
    "                        bitChanges = xorNum.encode().count(b'1')\n",
    "\n",
    "                        # if this is the current permutation level desired\n",
    "                        if(bitChanges == permLevel):\n",
    "                            n_buckets += 1\n",
    "                            retrieval.update(table[hash_table_idx])\n",
    "                        \n",
    "                        if(len(retrieval) >= max_results):\n",
    "                            break\n",
    "                    \n",
    "                    if(len(retrieval) >= max_results):\n",
    "                            break\n",
    "                # Original hash neighbors\n",
    "                else:\n",
    "                    if(table[binary_idx]):\n",
    "                        n_buckets += 1\n",
    "                        #print(\"Bucket {} : {}\".format(n_buckets, len(table[binary_idx])))\n",
    "                        retrieval.update(table[binary_idx])\n",
    "                    \n",
    "                    \n",
    "            permLevel += 1\n",
    "            \n",
    "            # No More Permutations left\n",
    "            if(permLevel == self.k):\n",
    "                break\n",
    "            \n",
    "        \n",
    "        retrieval = list(retrieval)\n",
    "        sim_scores = cosine_similarity(np.expand_dims(data_point, 0), self.data[retrieval]).ravel()\n",
    "        data_idx = sim_scores.argsort()[::-1][:max_results]\n",
    "        #print(sim_scores)\n",
    "        #print(data_idx)\n",
    "        #print(retrieval)\n",
    "        assert len(retrieval) == len(sim_scores)\n",
    "        return {'n_buckets': n_buckets, \n",
    "                'n_candidates': len(retrieval),\n",
    "                'scores': sim_scores[data_idx],\n",
    "                'retrieved_files': [self.idx_2_file[retrieval[d]] for d in data_idx]\n",
    "               }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh = LSH(L=8, k=4, mode='tfidf', input_dir='phase2_outputs/task0b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_buckets': 117,\n",
       " 'n_candidates': 93,\n",
       " 'scores': array([1.        , 0.27214871, 0.260384  , 0.25065707, 0.24599183,\n",
       "        0.24595429, 0.24285738, 0.23327691, 0.23297201, 0.23057715,\n",
       "        0.22743643, 0.22290228, 0.22274283, 0.22006634, 0.21413458,\n",
       "        0.21133903, 0.20892665, 0.20834708, 0.20325307, 0.20285688,\n",
       "        0.1996427 , 0.19601594, 0.19575157, 0.19420888, 0.1937912 ,\n",
       "        0.19208847, 0.19072984, 0.19056732, 0.18780037, 0.18347695,\n",
       "        0.18238245, 0.17984276, 0.17706218, 0.17701715, 0.17405459,\n",
       "        0.17159915, 0.16711412, 0.16649461, 0.16562823, 0.16365316,\n",
       "        0.16235872, 0.15999322, 0.15781616, 0.15697307, 0.15330541,\n",
       "        0.15195794, 0.15083902, 0.14988838, 0.14834812, 0.14626289,\n",
       "        0.14543055, 0.14542234, 0.14404824, 0.14383728, 0.14338521,\n",
       "        0.14178141, 0.14047743, 0.13981319, 0.13954217, 0.13902027,\n",
       "        0.13573397, 0.1339465 , 0.13116409, 0.13106888, 0.13075548,\n",
       "        0.13070178, 0.12641772, 0.12630289, 0.12520483, 0.12164735,\n",
       "        0.12077094, 0.11931274, 0.1181888 , 0.11576481, 0.11214967,\n",
       "        0.10940262, 0.10930039, 0.10873502, 0.10639115, 0.10567064,\n",
       "        0.10464336, 0.10213959, 0.09961313, 0.0976449 , 0.09742792,\n",
       "        0.09615826, 0.09524919, 0.0919147 , 0.08931329, 0.08905615,\n",
       "        0.07790274, 0.07703683, 0.07206737]),\n",
       " 'retrieved_files': ['phase2_outputs/task0b\\\\tfidf_vectors_010.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_579.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_018.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_563.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_571.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_021.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_005.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_259.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_013.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_561.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_568.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_560.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_567.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_007.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_250.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_249.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_269.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_251.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_565.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_264.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_003.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_588.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_562.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_578.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_582.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_257.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_252.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_574.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_255.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_569.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_009.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_575.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_573.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_589.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_577.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_016.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_564.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_008.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_019.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_014.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_261.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_260.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_559.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_572.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_576.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_566.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_580.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_258.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_583.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_268.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_012.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_587.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_006.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_271.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_267.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_570.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_011.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_270.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_266.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_263.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_256.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_278.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_581.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_253.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_024.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_254.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_031.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_586.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_004.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_029.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_279.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_002.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_001.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_025.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_027.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_026.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_277.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_584.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_262.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_265.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_020.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_585.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_022.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_274.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_023.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_273.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_030.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_275.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_028.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_276.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_017.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_015.txt',\n",
       "  'phase2_outputs/task0b\\\\tfidf_vectors_272.txt']}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_idx = 10\n",
    "lsh.query(np.array(data[query_idx]), max_results=98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
